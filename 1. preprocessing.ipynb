{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4aeec5a-eaec-4146-b685-41e51d8e7850",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c665589-3362-4d46-b360-f7581a3b1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab40b72f-b4c2-49ba-b50f-814c8acf7996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê²½ë¡œ ë° ì €ì¥ íŒŒì¼ëª… ì„¤ì •\n",
    "input_folder = './dataset/1.Training'\n",
    "save_folder = './dataset'\n",
    "output_filename = 'merged_training.json'\n",
    "output_file = os.path.join(save_folder, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff646d-9d47-4fe4-a3e3-c7f11a222911",
   "metadata": {},
   "source": [
    "### Training set ë³‘í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e438a1c-5b8c-48fd-8ab9-17f827706273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ './dataset/1.Training' ê²½ë¡œì—ì„œ ì´ 6ê°œì˜ JSON íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:15<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "âœ… [Training] ë°ì´í„° ë³‘í•© ì™„ë£Œ\n",
      "   - ì´ ë°ì´í„° ê±´ìˆ˜: 197,698 ê±´\n",
      "   - ë°œê²¬ëœ ì»¬ëŸ¼(Keys): ['doc_id', 'doc_title', 'doc_source', 'doc_published', 'doc_class', 'created', 'paragraphs']\n",
      "==================================================\n",
      "\n",
      "[ê²°ì¸¡ì¹˜(Null) ë³´ìœ  ì»¬ëŸ¼ ë° ê°œìˆ˜]\n",
      "âœ¨ ê²°ì¸¡ì¹˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (All Clean).\n",
      "\n",
      "ğŸ’¾ ë³‘í•©ëœ íŒŒì¼ì´ './dataset\\merged_training.json' ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>doc_source</th>\n",
       "      <th>doc_published</th>\n",
       "      <th>doc_class</th>\n",
       "      <th>created</th>\n",
       "      <th>paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D0000042870615</td>\n",
       "      <td>\"ê°•ë¶êµ¬ì˜íšŒ ì•„ë™í•™ëŒ€ì˜ˆë°© ë° ì•„ë™ë³µì§€ íŠ¹ë³„ìœ„ì›íšŒ, ê°•ë¶êµ¬ìœ¡ì•„ì¢…í•©ì§€ì›ì„¼í„° í˜„ì¥ í™œë™\"</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œì²­</td>\n",
       "      <td>20210626</td>\n",
       "      <td>{'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ê³µê³µí–‰ì •'}</td>\n",
       "      <td>20220422145553</td>\n",
       "      <td>[{'context': 'ì œëª©: ê°•ë¶êµ¬ì˜íšŒ ì•„ë™í•™ëŒ€ì˜ˆë°© ë° ì•„ë™ë³µì§€ íŠ¹ë³„ìœ„ì›íšŒ, ê°•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D0000042838802</td>\n",
       "      <td>6ì›” ì‹œë¯¼ ì†Œí†µ ì˜ìƒ ì œì‘ ê³„íš</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œì²­</td>\n",
       "      <td>20210625</td>\n",
       "      <td>{'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ê³µê³µí–‰ì •'}</td>\n",
       "      <td>20220422145553</td>\n",
       "      <td>[{'context': '6ì›” ì‹œë¯¼ ì†Œí†µ ì˜ìƒ ì œì‘ ê³„íš\n",
       "ï€€ 2021ë…„ ì¶”ì§„ ì‹¤ì (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0000042870612</td>\n",
       "      <td>2021ë…„ ì œ1ì°¨ ê°•ë¶êµ¬ ì¹˜ë§¤ì•ˆì‹¬ì„¼í„° ìë¬¸ìœ„ì›íšŒ ì„œë©´ ê°œìµœ</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œì²­</td>\n",
       "      <td>20210626</td>\n",
       "      <td>{'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ì‹í’ˆê±´ê°•'}</td>\n",
       "      <td>20220422145553</td>\n",
       "      <td>[{'context': 'ì œëª© 2021ë…„ ì œ1ì°¨ ê°•ë¶êµ¬ ì¹˜ë§¤ì•ˆì‹¬ì„¼í„° ìë¬¸ìœ„ì›íšŒ ì„œë©´...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_id                                       doc_title doc_source  \\\n",
       "0  D0000042870615  \"ê°•ë¶êµ¬ì˜íšŒ ì•„ë™í•™ëŒ€ì˜ˆë°© ë° ì•„ë™ë³µì§€ íŠ¹ë³„ìœ„ì›íšŒ, ê°•ë¶êµ¬ìœ¡ì•„ì¢…í•©ì§€ì›ì„¼í„° í˜„ì¥ í™œë™\"     ì„œìš¸íŠ¹ë³„ì‹œì²­   \n",
       "1  D0000042838802                               6ì›” ì‹œë¯¼ ì†Œí†µ ì˜ìƒ ì œì‘ ê³„íš     ì„œìš¸íŠ¹ë³„ì‹œì²­   \n",
       "2  D0000042870612                2021ë…„ ì œ1ì°¨ ê°•ë¶êµ¬ ì¹˜ë§¤ì•ˆì‹¬ì„¼í„° ìë¬¸ìœ„ì›íšŒ ì„œë©´ ê°œìµœ     ì„œìš¸íŠ¹ë³„ì‹œì²­   \n",
       "\n",
       "   doc_published                                 doc_class         created  \\\n",
       "0       20210626  {'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ê³µê³µí–‰ì •'}  20220422145553   \n",
       "1       20210625  {'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ê³µê³µí–‰ì •'}  20220422145553   \n",
       "2       20210626  {'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ì‹í’ˆê±´ê°•'}  20220422145553   \n",
       "\n",
       "                                          paragraphs  \n",
       "0  [{'context': 'ì œëª©: ê°•ë¶êµ¬ì˜íšŒ ì•„ë™í•™ëŒ€ì˜ˆë°© ë° ì•„ë™ë³µì§€ íŠ¹ë³„ìœ„ì›íšŒ, ê°•...  \n",
       "1  [{'context': '6ì›” ì‹œë¯¼ ì†Œí†µ ì˜ìƒ ì œì‘ ê³„íš\n",
       "ï€€ 2021ë…„ ì¶”ì§„ ì‹¤ì (...  \n",
       "2  [{'context': 'ì œëª© 2021ë…„ ì œ1ì°¨ ê°•ë¶êµ¬ ì¹˜ë§¤ì•ˆì‹¬ì„¼í„° ìë¬¸ìœ„ì›íšŒ ì„œë©´...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. íŒŒì¼ ë¦¬ìŠ¤íŠ¸ í™•ë³´ ë° ë³‘í•©\n",
    "all_data = []\n",
    "file_list = glob.glob(os.path.join(input_folder, '*.json'))\n",
    "\n",
    "print(f\"ğŸ“‚ '{input_folder}' ê²½ë¡œì—ì„œ ì´ {len(file_list)}ê°œì˜ JSON íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "for file_path in tqdm(file_list, desc=\"Processing Training Files\"):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Case A: íŒŒì¼ ë‚´ìš©ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ëŒ€ë¶€ë¶„ì˜ AI Hub ë°ì´í„°)\n",
    "            if isinstance(data, list):\n",
    "                all_data.extend(data)\n",
    "                \n",
    "            # Case B: íŒŒì¼ ë‚´ìš©ì´ ë”•ì…”ë„ˆë¦¬ì´ê³ , ë‚´ë¶€ì— ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°\n",
    "            elif isinstance(data, dict):\n",
    "                # 'data', 'body' ë“±ì˜ í‚¤ë¥¼ í™•ì¸í•˜ê±°ë‚˜, ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ í•­ëª©ì„ ì°¾ìŒ\n",
    "                found_list = False\n",
    "                for key, value in data.items():\n",
    "                    if isinstance(value, list):\n",
    "                        all_data.extend(value)\n",
    "                        found_list = True\n",
    "                        break\n",
    "                # ë¦¬ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë”•ì…”ë„ˆë¦¬ ë‹¨ì¼ í•­ëª©ìœ¼ë¡œ ê°„ì£¼\n",
    "                if not found_list:\n",
    "                    all_data.append(data)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading {file_path}: {e}\")\n",
    "\n",
    "# 2. DataFrame ë³€í™˜ ë° ê²°ì¸¡ì¹˜ ê²€ì‚¬\n",
    "df_train = pd.DataFrame(all_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"âœ… [Training] ë°ì´í„° ë³‘í•© ì™„ë£Œ\")\n",
    "print(f\"   - ì´ ë°ì´í„° ê±´ìˆ˜: {len(df_train):,} ê±´\")\n",
    "print(f\"   - ë°œê²¬ëœ ì»¬ëŸ¼(Keys): {list(df_train.columns)}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ê²°ì¸¡ì¹˜(Null) í™•ì¸\n",
    "print(\"\\n[ê²°ì¸¡ì¹˜(Null) ë³´ìœ  ì»¬ëŸ¼ ë° ê°œìˆ˜]\")\n",
    "null_counts = df_train.isnull().sum()\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"âœ¨ ê²°ì¸¡ì¹˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (All Clean).\")\n",
    "else:\n",
    "    print(null_counts[null_counts > 0])\n",
    "\n",
    "# 3. ë³‘í•©ëœ íŒŒì¼ ì €ì¥\n",
    "df_train.to_json(output_file, orient='records', force_ascii=False, indent=4)\n",
    "print(f\"\\nğŸ’¾ ë³‘í•©ëœ íŒŒì¼ì´ '{output_file}' ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë°ì´í„° ìƒ˜í”Œ í™•ì¸ (ìƒìœ„ 3ê°œ)\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db86288-df95-4d87-8d01-1fb0ca2021bd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3073b37f-d648-46e1-8b4e-b51d90c4bb26",
   "metadata": {},
   "source": [
    "### Validation set ë³‘í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc3a9336-96ba-4fc9-9325-257595ac28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê²½ë¡œ ë° ì €ì¥ íŒŒì¼ëª… ì„¤ì •\n",
    "input_folder = './dataset/2.Validation'\n",
    "save_folder = './dataset'\n",
    "output_filename = 'merged_validation.json'\n",
    "output_file = os.path.join(save_folder, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c11b0b-7a58-44ab-944f-e5e6289c83f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ './dataset/2.Validation' ê²½ë¡œì—ì„œ ì´ 6ê°œì˜ JSON íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "âœ… [Validation] ë°ì´í„° ë³‘í•© ì™„ë£Œ\n",
      "   - ì´ ë°ì´í„° ê±´ìˆ˜: 37,042 ê±´\n",
      "   - ë°œê²¬ëœ ì»¬ëŸ¼(Keys): ['doc_id', 'doc_title', 'doc_source', 'doc_published', 'doc_class', 'created', 'paragraphs']\n",
      "==================================================\n",
      "\n",
      "[ê²°ì¸¡ì¹˜(Null) ë³´ìœ  ì»¬ëŸ¼ ë° ê°œìˆ˜]\n",
      "âœ¨ ê²°ì¸¡ì¹˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (All Clean).\n",
      "\n",
      "ğŸ’¾ ë³‘í•©ëœ íŒŒì¼ì´ './dataset\\merged_validation.json' ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>doc_source</th>\n",
       "      <th>doc_published</th>\n",
       "      <th>doc_class</th>\n",
       "      <th>created</th>\n",
       "      <th>paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D0000042870615</td>\n",
       "      <td>\"ê°•ë¶êµ¬ì˜íšŒ ì•„ë™í•™ëŒ€ì˜ˆë°© ë° ì•„ë™ë³µì§€ íŠ¹ë³„ìœ„ì›íšŒ, ê°•ë¶êµ¬ìœ¡ì•„ì¢…í•©ì§€ì›ì„¼í„° í˜„ì¥ í™œë™\"</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œì²­</td>\n",
       "      <td>20210626</td>\n",
       "      <td>{'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ê³µê³µí–‰ì •'}</td>\n",
       "      <td>20220422150038</td>\n",
       "      <td>[{'context': 'ì œëª©: ê°•ë¶êµ¬ì˜íšŒ ì•„ë™í•™ëŒ€ì˜ˆë°© ë° ì•„ë™ë³µì§€ íŠ¹ë³„ìœ„ì›íšŒ, ê°•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D0000042870612</td>\n",
       "      <td>2021ë…„ ì œ1ì°¨ ê°•ë¶êµ¬ ì¹˜ë§¤ì•ˆì‹¬ì„¼í„° ìë¬¸ìœ„ì›íšŒ ì„œë©´ ê°œìµœ</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œì²­</td>\n",
       "      <td>20210626</td>\n",
       "      <td>{'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ì‹í’ˆê±´ê°•'}</td>\n",
       "      <td>20220422150038</td>\n",
       "      <td>[{'context': 'ì œëª© 2021ë…„ ì œ1ì°¨ ê°•ë¶êµ¬ ì¹˜ë§¤ì•ˆì‹¬ì„¼í„° ìë¬¸ìœ„ì›íšŒ ì„œë©´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0000042793509</td>\n",
       "      <td>2021ë…„ ì„œìš¸ì‹œ ì •ì±…ì†Œí†µí‰ê°€ë‹¨ ìš´ì˜ê³„íš</td>\n",
       "      <td>ì„œìš¸íŠ¹ë³„ì‹œì²­</td>\n",
       "      <td>20210618</td>\n",
       "      <td>{'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ê³µê³µí–‰ì •'}</td>\n",
       "      <td>20220422150038</td>\n",
       "      <td>[{'context': '2021ë…„ ì„œìš¸ì‹œ ì •ì±…ì†Œí†µí‰ê°€ë‹¨ ìš´ì˜ê³„íš\n",
       "ï€€ 2021ë…„ í‰...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_id                                       doc_title doc_source  \\\n",
       "0  D0000042870615  \"ê°•ë¶êµ¬ì˜íšŒ ì•„ë™í•™ëŒ€ì˜ˆë°© ë° ì•„ë™ë³µì§€ íŠ¹ë³„ìœ„ì›íšŒ, ê°•ë¶êµ¬ìœ¡ì•„ì¢…í•©ì§€ì›ì„¼í„° í˜„ì¥ í™œë™\"     ì„œìš¸íŠ¹ë³„ì‹œì²­   \n",
       "1  D0000042870612                2021ë…„ ì œ1ì°¨ ê°•ë¶êµ¬ ì¹˜ë§¤ì•ˆì‹¬ì„¼í„° ìë¬¸ìœ„ì›íšŒ ì„œë©´ ê°œìµœ     ì„œìš¸íŠ¹ë³„ì‹œì²­   \n",
       "2  D0000042793509                          2021ë…„ ì„œìš¸ì‹œ ì •ì±…ì†Œí†µí‰ê°€ë‹¨ ìš´ì˜ê³„íš     ì„œìš¸íŠ¹ë³„ì‹œì²­   \n",
       "\n",
       "   doc_published                                 doc_class         created  \\\n",
       "0       20210626  {'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ê³µê³µí–‰ì •'}  20220422150038   \n",
       "1       20210626  {'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ì‹í’ˆê±´ê°•'}  20220422150038   \n",
       "2       20210618  {'class': 'ì¤‘ì•™í–‰ì •ê¸°ê´€ ë¶„ë¥˜ì²´ê³„', 'code': 'ê³µê³µí–‰ì •'}  20220422150038   \n",
       "\n",
       "                                          paragraphs  \n",
       "0  [{'context': 'ì œëª©: ê°•ë¶êµ¬ì˜íšŒ ì•„ë™í•™ëŒ€ì˜ˆë°© ë° ì•„ë™ë³µì§€ íŠ¹ë³„ìœ„ì›íšŒ, ê°•...  \n",
       "1  [{'context': 'ì œëª© 2021ë…„ ì œ1ì°¨ ê°•ë¶êµ¬ ì¹˜ë§¤ì•ˆì‹¬ì„¼í„° ìë¬¸ìœ„ì›íšŒ ì„œë©´...  \n",
       "2  [{'context': '2021ë…„ ì„œìš¸ì‹œ ì •ì±…ì†Œí†µí‰ê°€ë‹¨ ìš´ì˜ê³„íš\n",
       "ï€€ 2021ë…„ í‰...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. íŒŒì¼ ë¦¬ìŠ¤íŠ¸ í™•ë³´ ë° ë³‘í•©\n",
    "all_data_val = []\n",
    "file_list_val = glob.glob(os.path.join(input_folder, '*.json'))\n",
    "\n",
    "print(f\"ğŸ“‚ '{input_folder}' ê²½ë¡œì—ì„œ ì´ {len(file_list_val)}ê°œì˜ JSON íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "for file_path in tqdm(file_list_val, desc=\"Processing Validation Files\"):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            if isinstance(data, list):\n",
    "                all_data_val.extend(data)\n",
    "            elif isinstance(data, dict):\n",
    "                found_list = False\n",
    "                for key, value in data.items():\n",
    "                    if isinstance(value, list):\n",
    "                        all_data_val.extend(value)\n",
    "                        found_list = True\n",
    "                        break\n",
    "                if not found_list:\n",
    "                    all_data_val.append(data)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading {file_path}: {e}\")\n",
    "\n",
    "\n",
    "# 2. DataFrame ë³€í™˜ ë° ê²°ì¸¡ì¹˜ ê²€ì‚¬\n",
    "df_val = pd.DataFrame(all_data_val)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"âœ… [Validation] ë°ì´í„° ë³‘í•© ì™„ë£Œ\")\n",
    "print(f\"   - ì´ ë°ì´í„° ê±´ìˆ˜: {len(df_val):,} ê±´\")\n",
    "print(f\"   - ë°œê²¬ëœ ì»¬ëŸ¼(Keys): {list(df_val.columns)}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ê²°ì¸¡ì¹˜(Null) í™•ì¸\n",
    "print(\"\\n[ê²°ì¸¡ì¹˜(Null) ë³´ìœ  ì»¬ëŸ¼ ë° ê°œìˆ˜]\")\n",
    "null_counts_val = df_val.isnull().sum()\n",
    "if null_counts_val.sum() == 0:\n",
    "    print(\"âœ¨ ê²°ì¸¡ì¹˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (All Clean).\")\n",
    "else:\n",
    "    print(null_counts_val[null_counts_val > 0])\n",
    "\n",
    "\n",
    "# 3. ë³‘í•©ëœ íŒŒì¼ ì €ì¥\n",
    "df_val.to_json(output_file, orient='records', force_ascii=False, indent=4)\n",
    "print(f\"\\nğŸ’¾ ë³‘í•©ëœ íŒŒì¼ì´ '{output_file}' ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë°ì´í„° ìƒ˜í”Œ í™•ì¸ (ìƒìœ„ 3ê°œ)\n",
    "df_val.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0db542-9006-45ff-a89c-8edfb4c88008",
   "metadata": {},
   "source": [
    "### ìƒì„±ëœ íŒŒì¼ ë³µì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925efe6b-3547-4ee0-a968-048f44fb8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë°œì§€ í´ë”\n",
    "source_folder = './dataset'\n",
    "\n",
    "# ëª©ì ì§€ í´ë”\n",
    "destination_folder = './results'\n",
    "\n",
    "# ë³µì‚¬í•  íŒŒì¼ëª…\n",
    "files_to_copy = ['merged_training.json', 'merged_validation.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1253c385-ab03-4302-8a12-b36fb9898fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Created directory: ./results\n",
      "ğŸš€ íŒŒì¼ ë³µì‚¬ ì‹œì‘: ./dataset -> ./results\n",
      "âœ… ë³µì‚¬ ì™„ë£Œ: merged_training.json\n",
      "âœ… ë³µì‚¬ ì™„ë£Œ: merged_validation.json\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 1. ëª©ì ì§€ í´ë” ìƒì„± (ì—†ì„ ê²½ìš°)\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "    print(f\"ğŸ“‚ Created directory: {destination_folder}\")\n",
    "\n",
    "# 2. íŒŒì¼ ì´ë™ ì‹¤í–‰\n",
    "print(f\"ğŸš€ íŒŒì¼ ë³µì‚¬ ì‹œì‘: {source_folder} -> {destination_folder}\")\n",
    "\n",
    "for file_name in files_to_copy:\n",
    "    src_path = os.path.join(source_folder, file_name)\n",
    "    dst_path = os.path.join(destination_folder, file_name)\n",
    "    \n",
    "    # ì†ŒìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "    if os.path.exists(src_path):\n",
    "        try:\n",
    "            # íŒŒì¼ ì´ë™\n",
    "            shutil.copy(src_path, dst_path)\n",
    "            print(f\"âœ… ë³µì‚¬ ì™„ë£Œ: {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë³µì‚¬ ì‹¤íŒ¨ ({file_name}): {e}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {src_path} (ì´ì „ ë‹¨ê³„ê°€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”)\")\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce903e64-4428-447c-8745-5eb836b0d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ì²˜ë¦¬ ì‹œì‘: ./results/merged_training.json\n",
      "ğŸ“‚ í´ë” ìƒì„±ë¨: ./results/cleaned\n",
      "âœ… ë³€í™˜ ì™„ë£Œ ë° ì €ì¥: ./results/cleaned/bert_train.json\n",
      "\n",
      "ğŸ“Š [Training Set] ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\n",
      "   - ìƒíƒœ: completed\n",
      "   - ì´ ë¬¸ì„œ ìˆ˜: 197698ê°œ\n",
      "   - ì´ ì§ˆë¬¸ ìˆ˜: 329464ê°œ\n",
      "   - âœ… ë³€í™˜ ì„±ê³µ: 292952ê°œ\n",
      "   - â›” ì œì™¸ë¨ (ì •ë‹µ ì¸ë±ìŠ¤ ëª» ì°¾ìŒ): 36512ê°œ\n",
      "   - â›” ì œì™¸ë¨ (ì •ë‹µ í…ìŠ¤íŠ¸ ë¹„ì–´ìˆìŒ): 0ê°œ\n",
      "----------------------------------------\n",
      "ğŸ”„ ì²˜ë¦¬ ì‹œì‘: ./results/merged_validation.json\n",
      "âœ… ë³€í™˜ ì™„ë£Œ ë° ì €ì¥: ./results/cleaned/bert_validation.json\n",
      "\n",
      "ğŸ“Š [Validation Set] ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\n",
      "   - ìƒíƒœ: completed\n",
      "   - ì´ ë¬¸ì„œ ìˆ˜: 37042ê°œ\n",
      "   - ì´ ì§ˆë¬¸ ìˆ˜: 41182ê°œ\n",
      "   - âœ… ë³€í™˜ ì„±ê³µ: 37254ê°œ\n",
      "   - â›” ì œì™¸ë¨ (ì •ë‹µ ì¸ë±ìŠ¤ ëª» ì°¾ìŒ): 3928ê°œ\n",
      "   - â›” ì œì™¸ë¨ (ì •ë‹µ í…ìŠ¤íŠ¸ ë¹„ì–´ìˆìŒ): 0ê°œ\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def convert_custom_to_korquad(input_file, output_file):\n",
    "    print(f\"ğŸ”„ ì²˜ë¦¬ ì‹œì‘: {input_file}\")\n",
    "    \n",
    "    # í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "    stats = {\n",
    "        \"status\": \"processing\", # processing, completed, error\n",
    "        \"error_msg\": \"\",\n",
    "        \"total_docs\": 0,\n",
    "        \"total_qas\": 0,\n",
    "        \"valid_qas\": 0,\n",
    "        \"skipped_no_answer_index\": 0, # ë³¸ë¬¸ì—ì„œ ì •ë‹µì„ ëª» ì°¾ì€ ê²½ìš°\n",
    "        \"skipped_empty_answer\": 0     # ì •ë‹µ í…ìŠ¤íŠ¸ ìì²´ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°\n",
    "    }\n",
    "    \n",
    "    # 1. ì…ë ¥ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "    if not os.path.exists(input_file):\n",
    "        stats[\"status\"] = \"error\"\n",
    "        stats[\"error_msg\"] = f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ ({input_file})\"\n",
    "        print(f\"âŒ [ì˜¤ë¥˜] {stats['error_msg']}\")\n",
    "        return stats\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            source_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        stats[\"status\"] = \"error\"\n",
    "        stats[\"error_msg\"] = f\"JSON ë¡œë“œ ì‹¤íŒ¨ ({str(e)})\"\n",
    "        print(f\"âŒ [ì˜¤ë¥˜] {stats['error_msg']}\")\n",
    "        return stats\n",
    "\n",
    "    stats[\"total_docs\"] = len(source_data)\n",
    "    \n",
    "    korquad_data = {\n",
    "        \"version\": \"KorQuAD_v1.0_transformed\",\n",
    "        \"data\": []\n",
    "    }\n",
    "\n",
    "    # 2. ë°ì´í„° ë³€í™˜ ë£¨í”„\n",
    "    for doc in source_data:\n",
    "        k_doc = {\n",
    "            \"title\": doc.get('doc_title', 'No Title'),\n",
    "            \"paragraphs\": []\n",
    "        }\n",
    "        \n",
    "        for para in doc['paragraphs']:\n",
    "            context = para['context']\n",
    "            qas_list = []\n",
    "            \n",
    "            for qa in para['qas']:\n",
    "                stats[\"total_qas\"] += 1\n",
    "                \n",
    "                # ì •ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "                ans_obj = qa['answers']\n",
    "                answer_text = ans_obj.get('text', '').strip()\n",
    "                \n",
    "                # Case A: ì •ë‹µ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°\n",
    "                if not answer_text:\n",
    "                    stats[\"skipped_empty_answer\"] += 1\n",
    "                    continue\n",
    "\n",
    "                # Case B: ì •ë‹µ ì¸ë±ìŠ¤ ê²€ìƒ‰ (ë³¸ë¬¸ ë‚´ ìœ„ì¹˜ ì°¾ê¸°)\n",
    "                start_idx = context.find(answer_text)\n",
    "                \n",
    "                if start_idx != -1:\n",
    "                    # ì„±ê³µ: ìœ íš¨í•œ ë°ì´í„°ë¡œ ì¶”ê°€\n",
    "                    new_qa = {\n",
    "                        \"id\": qa['question_id'],\n",
    "                        \"question\": qa['question'],\n",
    "                        \"answers\": [\n",
    "                            {\n",
    "                                \"text\": answer_text,\n",
    "                                \"answer_start\": start_idx\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                    qas_list.append(new_qa)\n",
    "                    stats[\"valid_qas\"] += 1\n",
    "                else:\n",
    "                    # ì‹¤íŒ¨: ë³¸ë¬¸ì— ì •ë‹µ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°\n",
    "                    stats[\"skipped_no_answer_index\"] += 1\n",
    "            \n",
    "            # ìœ íš¨í•œ ì§ˆë¬¸ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ” ë¬¸ë‹¨ë§Œ ì €ì¥\n",
    "            if qas_list:\n",
    "                k_doc['paragraphs'].append({\n",
    "                    \"context\": context,\n",
    "                    \"qas\": qas_list\n",
    "                })\n",
    "        \n",
    "        # ìœ íš¨í•œ ë¬¸ë‹¨ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ” ë¬¸ì„œë§Œ ì €ì¥\n",
    "        if k_doc['paragraphs']:\n",
    "            korquad_data['data'].append(k_doc)\n",
    "\n",
    "    # 3. ê²°ê³¼ ì €ì¥ (í´ë” ìë™ ìƒì„± í¬í•¨)\n",
    "    try:\n",
    "        # ì¶œë ¥ ê²½ë¡œì˜ ë””ë ‰í† ë¦¬(í´ë”) ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ ìƒì„±\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            print(f\"ğŸ“‚ í´ë” ìƒì„±ë¨: {output_dir}\")\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(korquad_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        stats[\"status\"] = \"completed\"\n",
    "        print(f\"âœ… ë³€í™˜ ì™„ë£Œ ë° ì €ì¥: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        stats[\"status\"] = \"error\"\n",
    "        stats[\"error_msg\"] = f\"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ({str(e)})\"\n",
    "        print(f\"âŒ [ì˜¤ë¥˜] {stats['error_msg']}\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "def print_summary(file_label, stats):\n",
    "    print(f\"\\nğŸ“Š [{file_label}] ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\")\n",
    "    print(f\"   - ìƒíƒœ: {stats['status']}\")\n",
    "    if stats['status'] == 'error':\n",
    "        print(f\"   - ì›ì¸: {stats['error_msg']}\")\n",
    "    else:\n",
    "        print(f\"   - ì´ ë¬¸ì„œ ìˆ˜: {stats['total_docs']}ê°œ\")\n",
    "        print(f\"   - ì´ ì§ˆë¬¸ ìˆ˜: {stats['total_qas']}ê°œ\")\n",
    "        print(f\"   - âœ… ë³€í™˜ ì„±ê³µ: {stats['valid_qas']}ê°œ\")\n",
    "        print(f\"   - â›” ì œì™¸ë¨ (ì •ë‹µ ì¸ë±ìŠ¤ ëª» ì°¾ìŒ): {stats['skipped_no_answer_index']}ê°œ\")\n",
    "        print(f\"   - â›” ì œì™¸ë¨ (ì •ë‹µ í…ìŠ¤íŠ¸ ë¹„ì–´ìˆìŒ): {stats['skipped_empty_answer']}ê°œ\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# --- [ì„¤ì • ë° ì‹¤í–‰] ---\n",
    "\n",
    "# ì…ë ¥ ê²½ë¡œ (dataset í´ë”)\n",
    "input_train = './results/merged_training.json'\n",
    "input_val = './results/merged_validation.json'\n",
    "\n",
    "# ì¶œë ¥ ê²½ë¡œ (dataset/cleaned í´ë”)\n",
    "output_train = './results/cleaned/bert_train.json'\n",
    "output_val = './results/cleaned/bert_validation.json'\n",
    "\n",
    "# ì‹¤í–‰ ë° í†µê³„ ì¶œë ¥\n",
    "stats_train = convert_custom_to_korquad(input_train, output_train)\n",
    "print_summary(\"Training Set\", stats_train)\n",
    "\n",
    "stats_val = convert_custom_to_korquad(input_val, output_val)\n",
    "print_summary(\"Validation Set\", stats_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b7fe3-ea7e-4b22-877d-babd9151b31e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
