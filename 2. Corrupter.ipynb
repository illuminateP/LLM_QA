{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0384bc5e-73d3-40bf-887e-fbd6af84e322",
   "metadata": {},
   "source": [
    "### Ïò§Ïóº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0188ef3d-dfff-489d-ac94-08b3cadfd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d86da2-e92e-428d-937e-b7e74d6f1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ÏÑ§Ï†ï ---\n",
    "INPUT_FILE = \"./results/cleaned/bert_train.json\"\n",
    "OUTPUT_DIR = \"./results/corrupted\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef06958b-8d29-4831-a4f6-13fd49a8ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cite_start]ÌäπÏàòÎ¨∏Ïûê ÌíÄ [cite: 12, 19]\n",
    "SPECIAL_CHARS = ['!', '@', '#', '$', '%', '^', '&', '*']\n",
    "\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- 1. Deletion Noise Logic (ÏÇ≠Ï†ú) ---\n",
    "def get_deletion_noise(text, answer_text, answer_start, ratio=0.15):\n",
    "    # Ï†ïÎãµ ÌÖçÏä§Ìä∏ ÌååÏïÖ (ÏõêÎ≥∏ Ïú†ÏßÄ)\n",
    "    answer_end = answer_start + len(answer_text)\n",
    "    ans_text_original = text[answer_start:answer_end]\n",
    "    \n",
    "    # Ï†ïÎãµ Ïïû/Îí§ ÌÖçÏä§Ìä∏ Î∂ÑÎ¶¨\n",
    "    pre_text = text[:answer_start]\n",
    "    post_text = text[answer_end:]\n",
    "    \n",
    "    # Í≥µÎ∞± Í∏∞Ï§Ä ÌÜ†ÌÅ∞Ìôî\n",
    "    pre_tokens = pre_text.split(' ')\n",
    "    post_tokens = post_text.split(' ')\n",
    "    \n",
    "    def delete_tokens(tokens, r):\n",
    "        if not tokens: return []\n",
    "        # Îπà Î¨∏ÏûêÏó¥ ÌÜ†ÌÅ∞ÏùÄ Ïù∏Îç±Ïä§ Í≥ÑÏÇ∞ÏóêÏÑú Ï†úÏô∏\n",
    "        valid_indices = [i for i, t in enumerate(tokens) if t.strip()]\n",
    "        n_delete = int(len(valid_indices) * r)\n",
    "        \n",
    "        if n_delete == 0: return tokens\n",
    "        \n",
    "        indices_to_delete = set(random.sample(valid_indices, n_delete))\n",
    "        return [t for i, t in enumerate(tokens) if i not in indices_to_delete]\n",
    "\n",
    "    new_pre_tokens = delete_tokens(pre_tokens, ratio)\n",
    "    new_post_tokens = delete_tokens(post_tokens, ratio)\n",
    "    \n",
    "    # Ïû¨Ï°∞Î¶Ω\n",
    "    new_pre_text = ' '.join(new_pre_tokens)\n",
    "    # Ïïû Î¨∏Ïû•Ïù¥ Ï§ÑÏñ¥Îì§ÏóàÍ±∞ÎÇò Ïú†ÏßÄÎê† Îïå, ÏõêÎ≥∏Ïù¥ Í≥µÎ∞±ÏúºÎ°ú ÎÅùÎÇ¨Îã§Î©¥ Í≥µÎ∞± Ïú†ÏßÄ\n",
    "    if len(new_pre_text) > 0 and not new_pre_text.endswith(' ') and pre_text.endswith(' '):\n",
    "        new_pre_text += ' '\n",
    "        \n",
    "    # [ÌïµÏã¨] Ï†ïÎãµ ÏãúÏûë ÏúÑÏπò Í∞±Ïã†: Î≥ÄÌòïÎêú ÏïûÎ∂ÄÎ∂Ñ ÌÖçÏä§Ìä∏Ïùò Í∏∏Ïù¥Î°ú Ïû¨ÏÑ§Ï†ï\n",
    "    new_answer_start = len(new_pre_text)\n",
    "    \n",
    "    new_post_text = ' '.join(new_post_tokens)\n",
    "    if len(new_post_text) > 0 and not new_post_text.startswith(' ') and post_text.startswith(' '):\n",
    "        new_post_text = ' ' + new_post_text\n",
    "        \n",
    "    new_context = new_pre_text + ans_text_original + new_post_text\n",
    "    \n",
    "    # ÏàòÏ†ïÎêú Î∂ÄÎ∂Ñ: Î∞òÌôòÍ∞í 3Í∞úÎ°ú ÎßûÏ∂§ (Context, Start Index, Answer Text)\n",
    "    return new_context, new_answer_start, ans_text_original\n",
    "\n",
    "# --- 2. Context Insertion Logic (ÏßÄÎ¨∏ Ïò§Ïóº) ---\n",
    "def get_context_insertion_noise(text, answer_text, answer_start, ratio=0.3):\n",
    "    answer_end = answer_start + len(answer_text)\n",
    "    \n",
    "    # Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôòÌïòÏó¨ Î¨∏Ïûê ÏÇΩÏûÖ\n",
    "    pre_text = list(text[:answer_start])\n",
    "    ans_text_original = text[answer_start:answer_end]\n",
    "    post_text = list(text[answer_end:])\n",
    "    \n",
    "    def insert_noise(char_list, r):\n",
    "        if not char_list: return \"\"\n",
    "        n_insert = int(len(char_list) * r)\n",
    "        for _ in range(n_insert):\n",
    "            pos = random.randint(0, len(char_list)) # ÎûúÎç§ ÏúÑÏπò ÏÇΩÏûÖ\n",
    "            char = random.choice(SPECIAL_CHARS)\n",
    "            char_list.insert(pos, char)\n",
    "        return \"\".join(char_list)\n",
    "\n",
    "    new_pre_text = insert_noise(pre_text, ratio)\n",
    "    new_post_text = insert_noise(post_text, ratio)\n",
    "    \n",
    "    # [ÌïµÏã¨] Ï†ïÎãµ ÏãúÏûë ÏúÑÏπò Í∞±Ïã†: ÎäòÏñ¥ÎÇú ÏïûÎ∂ÄÎ∂Ñ ÌÖçÏä§Ìä∏Ïùò Í∏∏Ïù¥Î°ú Ïû¨ÏÑ§Ï†ï\n",
    "    new_answer_start = len(new_pre_text)\n",
    "    new_context = new_pre_text + ans_text_original + new_post_text\n",
    "    \n",
    "    return new_context, new_answer_start, ans_text_original\n",
    "\n",
    "# --- 3. Answer Insertion Logic (Ï†ïÎãµ Ïò§Ïóº) ---\n",
    "def get_answer_insertion_noise(text, answer_text, answer_start, ratio=0.3):\n",
    "    answer_end = answer_start + len(answer_text)\n",
    "    \n",
    "    pre_text = text[:answer_start]\n",
    "    ans_chars = list(text[answer_start:answer_end])\n",
    "    post_text = text[answer_end:]\n",
    "    \n",
    "    n_insert = int(len(ans_chars) * ratio)\n",
    "    n_insert = max(1, n_insert)\n",
    "    \n",
    "    for _ in range(n_insert):\n",
    "        pos = random.randint(1, len(ans_chars)-1) if len(ans_chars) > 1 else 0\n",
    "        char = random.choice(SPECIAL_CHARS)\n",
    "        ans_chars.insert(pos, char)\n",
    "        \n",
    "    new_ans_text = \"\".join(ans_chars)\n",
    "    new_context = pre_text + new_ans_text + post_text\n",
    "    \n",
    "    # Ï†ïÎãµ ÎÇ¥Î∂ÄÍ∞Ä Î≥ÄÌïú Í≤ÉÏù¥ÎØÄÎ°ú ÏãúÏûë Ïù∏Îç±Ïä§(answer_start)Îäî Î≥ÄÌïòÏßÄ ÏïäÏùå\n",
    "    return new_context, answer_start, new_ans_text\n",
    "\n",
    "# --- ÌÜµÌï© ÏÉùÏÑ± Ìï®Ïàò ---\n",
    "def generate_all_datasets():\n",
    "    print(f\"üîÑ Loading input: {INPUT_FILE}\")\n",
    "    try:\n",
    "        source_data = load_json(INPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Input file not found. Please run the preprocessing step first.\")\n",
    "        return\n",
    "\n",
    "    datasets = {\n",
    "        \"baseline\":      {\"version\": \"KorQuAD_v1.0_baseline\", \"data\": []},\n",
    "        \"deletion\":      {\"version\": \"KorQuAD_v1.0_deletion\", \"data\": []},\n",
    "        \"insertion_que\": {\"version\": \"KorQuAD_v1.0_insertion_que\", \"data\": []},\n",
    "        \"insertion_ans\": {\"version\": \"KorQuAD_v1.0_insertion_ans\", \"data\": []}\n",
    "    }\n",
    "    \n",
    "    print(\"‚ö° Processing all noise types...\")\n",
    "    \n",
    "    for doc in tqdm(source_data['data']):\n",
    "        doc_structs = {\n",
    "            k: {\"title\": doc['title'], \"paragraphs\": []} for k in datasets.keys()\n",
    "        }\n",
    "        \n",
    "        for para in doc['paragraphs']:\n",
    "            original_context = para['context']\n",
    "            \n",
    "            for qa in para['qas']:\n",
    "                ans = qa['answers'][0]\n",
    "                a_text = ans['text']\n",
    "                a_start = ans['answer_start']\n",
    "                \n",
    "                # 1. Baseline\n",
    "                doc_structs[\"baseline\"][\"paragraphs\"].append({\n",
    "                    \"context\": original_context,\n",
    "                    \"qas\": [qa]\n",
    "                })\n",
    "\n",
    "                # 2. Deletion (Î∞òÌôòÍ∞í 3Í∞ú Îß§Ïπ≠ ÏàòÏ†ï ÏôÑÎ£å)\n",
    "                del_ctx, del_start, del_ans = get_deletion_noise(original_context, a_text, a_start, ratio=0.15)\n",
    "                doc_structs[\"deletion\"][\"paragraphs\"].append({\n",
    "                    \"context\": del_ctx,\n",
    "                    \"qas\": [{\n",
    "                        \"id\": qa['id'], \"question\": qa['question'],\n",
    "                        \"answers\": [{\"text\": del_ans, \"answer_start\": del_start}]\n",
    "                    }]\n",
    "                })\n",
    "                \n",
    "                # 3. Insertion Que\n",
    "                ins_q_ctx, ins_q_start, ins_q_ans = get_context_insertion_noise(original_context, a_text, a_start, ratio=0.3)\n",
    "                doc_structs[\"insertion_que\"][\"paragraphs\"].append({\n",
    "                    \"context\": ins_q_ctx,\n",
    "                    \"qas\": [{\n",
    "                        \"id\": qa['id'], \"question\": qa['question'],\n",
    "                        \"answers\": [{\"text\": ins_q_ans, \"answer_start\": ins_q_start}]\n",
    "                    }]\n",
    "                })\n",
    "                \n",
    "                # 4. Insertion Ans\n",
    "                ins_a_ctx, ins_a_start, ins_a_ans = get_answer_insertion_noise(original_context, a_text, a_start, ratio=0.3)\n",
    "                doc_structs[\"insertion_ans\"][\"paragraphs\"].append({\n",
    "                    \"context\": ins_a_ctx,\n",
    "                    \"qas\": [{\n",
    "                        \"id\": qa['id'], \"question\": qa['question'],\n",
    "                        \"answers\": [{\"text\": ins_a_ans, \"answer_start\": ins_a_start}]\n",
    "                    }]\n",
    "                })\n",
    "        \n",
    "        # Î¨∏ÏÑú Ï†ÄÏû•\n",
    "        for k in datasets.keys():\n",
    "            if doc_structs[k][\"paragraphs\"]:\n",
    "                datasets[k][\"data\"].append(doc_structs[k])\n",
    "\n",
    "    print(\"üíæ Saving files...\")\n",
    "    for key, data_content in datasets.items():\n",
    "        filename = f\"train_{key}.json\"\n",
    "        save_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        save_json(data_content, save_path)\n",
    "        print(f\"   - Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce55518-90d1-48d5-a5ab-a31a722b1f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading input: ./results/cleaned/bert_train.json\n",
      "‚ö° Processing all noise types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175807/175807 [07:17<00:00, 402.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving files...\n",
      "   - Saved: ./results/corrupted\\train_baseline.json\n",
      "   - Saved: ./results/corrupted\\train_deletion.json\n",
      "   - Saved: ./results/corrupted\\train_insertion_que.json\n",
      "   - Saved: ./results/corrupted\\train_insertion_ans.json\n"
     ]
    }
   ],
   "source": [
    "# Ïã§Ìñâ\n",
    "generate_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c6d4a-6fac-482a-8da2-6c88d760cabf",
   "metadata": {},
   "source": [
    "### Í≤ÄÏ¶ù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a831d380-dc4a-47e4-97d3-ce2027554aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Baseline Data for comparison...\n",
      "   Loaded 292952 reference QA pairs.\n",
      "\n",
      "\n",
      "üîé Verifying: train_baseline.json (Mode: baseline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175807/175807 [00:00<00:00, 194022.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Results for train_baseline.json\n",
      "   - Total Samples: 292952\n",
      "   - ‚úÖ Alignment OK: 292952 (100.0%)\n",
      "--------------------------------------------------\n",
      "\n",
      "üîé Verifying: train_deletion.json (Mode: deletion)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175807/175807 [00:01<00:00, 96190.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Results for train_deletion.json\n",
      "   - Total Samples: 292952\n",
      "   - ‚úÖ Alignment OK: 292952 (100.0%)\n",
      "   - üß™ Corruption Verified: 292941 (100.0%)\n",
      "     (‚ÑπÔ∏è Includes 6 samples skipped due to length < 7 words)\n",
      "   - ‚ö†Ô∏è Unchanged Samples (Fail): 11\n",
      "     (Note: Insertion failed on empty strings or other edge cases)\n",
      "--------------------------------------------------\n",
      "\n",
      "üîé Verifying: train_insertion_ans.json (Mode: insertion_ans)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175807/175807 [00:02<00:00, 74103.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Results for train_insertion_ans.json\n",
      "   - Total Samples: 292952\n",
      "   - ‚úÖ Alignment OK: 292952 (100.0%)\n",
      "   - üß™ Corruption Verified: 292952 (100.0%)\n",
      "   - ‚ö†Ô∏è Unchanged Samples (Fail): 0\n",
      "--------------------------------------------------\n",
      "\n",
      "üîé Verifying: train_insertion_que.json (Mode: insertion_que)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175807/175807 [00:02<00:00, 76371.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Results for train_insertion_que.json\n",
      "   - Total Samples: 292952\n",
      "   - ‚úÖ Alignment OK: 292952 (100.0%)\n",
      "   - üß™ Corruption Verified: 292952 (100.0%)\n",
      "   - ‚ö†Ô∏è Unchanged Samples (Fail): 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Í≤ÄÏ¶ù ÎåÄÏÉÅ Í≤ΩÎ°ú\n",
    "TARGET_DIR = \"./results/corrupted\"\n",
    "BASELINE_FILE = os.path.join(TARGET_DIR, \"train_baseline.json\")\n",
    "SPECIAL_CHARS = ['!', '@', '#', '$', '%', '^', '&', '*']\n",
    "\n",
    "def load_json(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Load Failed: {filepath} ({e})\")\n",
    "        return None\n",
    "\n",
    "def build_reference_map(baseline_data):\n",
    "    \"\"\"\n",
    "    ÏõêÎ≥∏(Baseline) Îç∞Ïù¥ÌÑ∞Î•º QA ID Í∏∞Ï§ÄÏúºÎ°ú Îπ†Î•¥Í≤å Ï°∞ÌöåÌïòÍ∏∞ ÏúÑÌïú Îßµ ÏÉùÏÑ±\n",
    "    \"\"\"\n",
    "    ref_map = {}\n",
    "    for doc in baseline_data['data']:\n",
    "        for para in doc['paragraphs']:\n",
    "            context = para['context']\n",
    "            for qa in para['qas']:\n",
    "                qid = qa['id']\n",
    "                ans = qa['answers'][0]\n",
    "                ref_map[qid] = {\n",
    "                    \"context\": context,\n",
    "                    \"text\": ans['text'],\n",
    "                    \"start\": ans['answer_start']\n",
    "                }\n",
    "    return ref_map\n",
    "\n",
    "def verify_dataset(filepath, mode, ref_map):\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"\\nüîé Verifying: {filename} (Mode: {mode})\")\n",
    "    \n",
    "    data = load_json(filepath)\n",
    "    if not data: return\n",
    "\n",
    "    stats = {\n",
    "        \"total\": 0,\n",
    "        \"alignment_ok\": 0,\n",
    "        \"alignment_fail\": 0,\n",
    "        \"corruption_ok\": 0,   # Ïò§ÏóºÏù¥ ÏùòÎèÑÎåÄÎ°ú Ï†ÅÏö©Îê® (ÎòêÎäî ÏùòÎèÑÎêú Ïä§ÌÇµ)\n",
    "        \"corruption_fail\": 0, # Ïò§ÏóºÏù¥ Ï†ÅÏö©ÎêòÏßÄ ÏïäÏùå (ÏàòÏπò Î≥ÄÌôî ÏóÜÏùå Îì±)\n",
    "        \"skipped_short\": 0,   # [Ï∂îÍ∞Ä] Í∏∏Ïù¥Í∞Ä ÏßßÏïÑ ÏÇ≠Ï†úÍ∞Ä ÏÉùÎûµÎêú Í±¥Ïàò\n",
    "        \"errors\": []\n",
    "    }\n",
    "\n",
    "    # Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏàúÌöå\n",
    "    for doc in tqdm(data['data'], desc=\"Checking samples\"):\n",
    "        for para in doc['paragraphs']:\n",
    "            context = para['context']\n",
    "            for qa in para['qas']:\n",
    "                stats[\"total\"] += 1\n",
    "                qid = qa['id']\n",
    "                \n",
    "                # ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Ïùò Ï†ïÎãµ Ï†ïÎ≥¥\n",
    "                curr_ans = qa['answers'][0]\n",
    "                curr_text = curr_ans['text']\n",
    "                curr_start = curr_ans['answer_start']\n",
    "\n",
    "                # 1. Alignment Check (ÏûêÍ∞Ä Í≤ÄÏ¶ù)\n",
    "                # Ïù∏Îç±Ïä§Î°ú ÏûòÎùºÎÇ∏ ÌÖçÏä§Ìä∏Í∞Ä Ï†ïÎãµ ÌÖçÏä§Ìä∏ÏôÄ ÏùºÏπòÌïòÎäîÏßÄ\n",
    "                extracted = context[curr_start : curr_start + len(curr_text)]\n",
    "                if extracted == curr_text:\n",
    "                    stats[\"alignment_ok\"] += 1\n",
    "                else:\n",
    "                    stats[\"alignment_fail\"] += 1\n",
    "                    if len(stats[\"errors\"]) < 3:\n",
    "                        stats[\"errors\"].append(f\"[Align Error] ID: {qid} | Expected: {curr_text} != Extracted: {extracted}\")\n",
    "\n",
    "                # 2. Corruption Check (ÏõêÎ≥∏ ÎπÑÍµê Í≤ÄÏ¶ù)\n",
    "                if mode == 'baseline':\n",
    "                    stats[\"corruption_ok\"] += 1 # BaselineÏùÄ Í≤ÄÏ¶ù ÎåÄÏÉÅ ÏïÑÎãò\n",
    "                    continue\n",
    "\n",
    "                if qid not in ref_map:\n",
    "                    continue\n",
    "                \n",
    "                ref = ref_map[qid]\n",
    "                is_corrupted = False\n",
    "                \n",
    "                if mode == 'deletion':\n",
    "                    # [ÏÇ≠Ï†ú Í≤ÄÏ¶ù A] ÏõêÎ≥∏Î≥¥Îã§ Context Í∏∏Ïù¥Í∞Ä ÏßßÏïÑÏ°åÎäîÏßÄ ÌôïÏù∏\n",
    "                    if len(context) < len(ref['context']):\n",
    "                        is_corrupted = True\n",
    "                    else:\n",
    "                        # [ÏÇ≠Ï†ú Í≤ÄÏ¶ù B - Ï∂îÍ∞ÄÎêú Î°úÏßÅ]\n",
    "                        # Í∏∏Ïù¥Í∞Ä Ïïà Ï§ÑÏóàÎã§Î©¥, ÏõêÎ≥∏Ïù¥ ÎÑàÎ¨¥ ÏßßÏïÑÏÑú(7Îã®Ïñ¥ ÎØ∏Îßå) Í∑∏Îü∞ Í≤ÉÏù∏ÏßÄ ÌôïÏù∏\n",
    "                        # ÏÉùÏÑ± ÏΩîÎìúÏùò Í∏∞Ï§Ä: int(word_count * 0.15) == 0 Ïù¥Î©¥ Ïä§ÌÇµÎê®\n",
    "                        ref_word_count = len(ref['context'].split())\n",
    "                        if ref_word_count < 7:\n",
    "                            # 7Îã®Ïñ¥ ÎØ∏ÎßåÏù¥Îùº ÏÇ≠Ï†ú Ïïà Îêú Í≤ÉÏù¥ÎØÄÎ°ú \"Ï†ïÏÉÅ ÎèôÏûë\"ÏúºÎ°ú Í∞ÑÏ£º\n",
    "                            is_corrupted = True \n",
    "                            stats[\"skipped_short\"] += 1\n",
    "                    \n",
    "                elif mode == 'insertion_que':\n",
    "                    # [cite_start][ÏßÄÎ¨∏ Ïò§Ïóº Í≤ÄÏ¶ù] Context Í∏∏Ïù¥Í∞Ä ÎäòÏñ¥ÎÇòÍ≥† ÌäπÏàòÎ¨∏ÏûêÍ∞Ä Ìè¨Ìï® [cite: 12]\n",
    "                    has_special = any(c in context for c in SPECIAL_CHARS)\n",
    "                    is_longer = len(context) > len(ref['context'])\n",
    "                    if has_special and is_longer:\n",
    "                        is_corrupted = True\n",
    "                        \n",
    "                elif mode == 'insertion_ans':\n",
    "                    # [cite_start][Ï†ïÎãµ Ïò§Ïóº Í≤ÄÏ¶ù] Ï†ïÎãµ Text Í∏∏Ïù¥Í∞Ä ÎäòÏñ¥ÎÇòÍ≥† ÌäπÏàòÎ¨∏Ïûê Ìè¨Ìï® [cite: 19]\n",
    "                    has_special = any(c in curr_text for c in SPECIAL_CHARS)\n",
    "                    is_longer = len(curr_text) > len(ref['text'])\n",
    "                    if has_special and is_longer:\n",
    "                        is_corrupted = True\n",
    "                \n",
    "                if is_corrupted:\n",
    "                    stats[\"corruption_ok\"] += 1\n",
    "                else:\n",
    "                    stats[\"corruption_fail\"] += 1\n",
    "\n",
    "    # --- Í≤∞Í≥º Ï∂úÎ†• ---\n",
    "    print(f\"   üìä Results for {filename}\")\n",
    "    print(f\"   - Total Samples: {stats['total']}\")\n",
    "    print(f\"   - ‚úÖ Alignment OK: {stats['alignment_ok']} ({(stats['alignment_ok']/stats['total'])*100:.1f}%)\")\n",
    "    \n",
    "    if stats['alignment_fail'] > 0:\n",
    "        print(f\"   - ‚ùå Alignment Failed: {stats['alignment_fail']}\")\n",
    "        for err in stats['errors']:\n",
    "            print(f\"      -> {err}\")\n",
    "            \n",
    "    if mode != 'baseline':\n",
    "        print(f\"   - üß™ Corruption Verified: {stats['corruption_ok']} ({(stats['corruption_ok']/stats['total'])*100:.1f}%)\")\n",
    "        \n",
    "        # [Ï∂îÍ∞ÄÎêú Ï∂úÎ†•] ÏßßÏùÄ Î¨∏Ïû• Ïä§ÌÇµ ÌÜµÍ≥Ñ ÌëúÏãú\n",
    "        if mode == 'deletion' and stats['skipped_short'] > 0:\n",
    "             print(f\"     (‚ÑπÔ∏è Includes {stats['skipped_short']} samples skipped due to length < 7 words)\")\n",
    "\n",
    "        print(f\"   - ‚ö†Ô∏è Unchanged Samples (Fail): {stats['corruption_fail']}\")\n",
    "        if stats['corruption_fail'] > 0:\n",
    "            print(\"     (Note: Insertion failed on empty strings or other edge cases)\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# --- Î©îÏù∏ Ïã§Ìñâ Î°úÏßÅ ---\n",
    "def main():\n",
    "    if not os.path.exists(BASELINE_FILE):\n",
    "        print(\"‚ùå Baseline file not found. Please run corrupter.ipynb first.\")\n",
    "        return\n",
    "\n",
    "    print(\"üì• Loading Baseline Data for comparison...\")\n",
    "    baseline_data = load_json(BASELINE_FILE)\n",
    "    ref_map = build_reference_map(baseline_data)\n",
    "    print(f\"   Loaded {len(ref_map)} reference QA pairs.\\n\")\n",
    "\n",
    "    # glob.glob ÏÇ¨Ïö©ÏúºÎ°ú ÏàòÏ†ï\n",
    "    files = glob(os.path.join(TARGET_DIR, \"train_*.json\"))\n",
    "    files.sort()\n",
    "\n",
    "    for file_path in files:\n",
    "        mode = os.path.basename(file_path).replace(\"train_\", \"\").replace(\".json\", \"\")\n",
    "        verify_dataset(file_path, mode, ref_map)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6803ce-7216-4b2a-ab6d-a74d315a246c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
