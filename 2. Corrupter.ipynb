{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0384bc5e-73d3-40bf-887e-fbd6af84e322",
   "metadata": {},
   "source": [
    "### ì˜¤ì—¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0188ef3d-dfff-489d-ac94-08b3cadfd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d86da2-e92e-428d-937e-b7e74d6f1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ì„¤ì • ---\n",
    "INPUT_FILE = \"./results/cleaned/bert_train.json\"\n",
    "OUTPUT_DIR = \"./results/corrupted\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef06958b-8d29-4831-a4f6-13fd49a8ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cite_start]íŠ¹ìˆ˜ë¬¸ì í’€ [cite: 12, 19]\n",
    "SPECIAL_CHARS = ['!', '@', '#', '$', '%', '^', '&', '*']\n",
    "\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- 1. Deletion Noise Logic (ì‚­ì œ) ---\n",
    "def get_deletion_noise(text, answer_text, answer_start, ratio=0.15):\n",
    "    # ì •ë‹µ í…ìŠ¤íŠ¸ íŒŒì•… (ì›ë³¸ ìœ ì§€)\n",
    "    answer_end = answer_start + len(answer_text)\n",
    "    ans_text_original = text[answer_start:answer_end]\n",
    "    \n",
    "    # ì •ë‹µ ì•/ë’¤ í…ìŠ¤íŠ¸ ë¶„ë¦¬\n",
    "    pre_text = text[:answer_start]\n",
    "    post_text = text[answer_end:]\n",
    "    \n",
    "    # ê³µë°± ê¸°ì¤€ í† í°í™”\n",
    "    pre_tokens = pre_text.split(' ')\n",
    "    post_tokens = post_text.split(' ')\n",
    "    \n",
    "    def delete_tokens(tokens, r):\n",
    "        if not tokens: return []\n",
    "        # ë¹ˆ ë¬¸ìì—´ í† í°ì€ ì¸ë±ìŠ¤ ê³„ì‚°ì—ì„œ ì œì™¸\n",
    "        valid_indices = [i for i, t in enumerate(tokens) if t.strip()]\n",
    "        n_delete = int(len(valid_indices) * r)\n",
    "        \n",
    "        if n_delete == 0: return tokens\n",
    "        \n",
    "        indices_to_delete = set(random.sample(valid_indices, n_delete))\n",
    "        return [t for i, t in enumerate(tokens) if i not in indices_to_delete]\n",
    "\n",
    "    new_pre_tokens = delete_tokens(pre_tokens, ratio)\n",
    "    new_post_tokens = delete_tokens(post_tokens, ratio)\n",
    "    \n",
    "    # ì¬ì¡°ë¦½\n",
    "    new_pre_text = ' '.join(new_pre_tokens)\n",
    "    # ì• ë¬¸ì¥ì´ ì¤„ì–´ë“¤ì—ˆê±°ë‚˜ ìœ ì§€ë  ë•Œ, ì›ë³¸ì´ ê³µë°±ìœ¼ë¡œ ëë‚¬ë‹¤ë©´ ê³µë°± ìœ ì§€\n",
    "    if len(new_pre_text) > 0 and not new_pre_text.endswith(' ') and pre_text.endswith(' '):\n",
    "        new_pre_text += ' '\n",
    "        \n",
    "    # [í•µì‹¬] ì •ë‹µ ì‹œì‘ ìœ„ì¹˜ ê°±ì‹ : ë³€í˜•ëœ ì•ë¶€ë¶„ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¡œ ì¬ì„¤ì •\n",
    "    new_answer_start = len(new_pre_text)\n",
    "    \n",
    "    new_post_text = ' '.join(new_post_tokens)\n",
    "    if len(new_post_text) > 0 and not new_post_text.startswith(' ') and post_text.startswith(' '):\n",
    "        new_post_text = ' ' + new_post_text\n",
    "        \n",
    "    new_context = new_pre_text + ans_text_original + new_post_text\n",
    "    \n",
    "    # ìˆ˜ì •ëœ ë¶€ë¶„: ë°˜í™˜ê°’ 3ê°œë¡œ ë§ì¶¤ (Context, Start Index, Answer Text)\n",
    "    return new_context, new_answer_start, ans_text_original\n",
    "\n",
    "# --- 2. Context Insertion Logic (ì§€ë¬¸ ì˜¤ì—¼) ---\n",
    "def get_context_insertion_noise(text, answer_text, answer_start, ratio=0.3):\n",
    "    answer_end = answer_start + len(answer_text)\n",
    "    \n",
    "    # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë¬¸ì ì‚½ì…\n",
    "    pre_text = list(text[:answer_start])\n",
    "    ans_text_original = text[answer_start:answer_end]\n",
    "    post_text = list(text[answer_end:])\n",
    "    \n",
    "    def insert_noise(char_list, r):\n",
    "        if not char_list: return \"\"\n",
    "        n_insert = int(len(char_list) * r)\n",
    "        for _ in range(n_insert):\n",
    "            pos = random.randint(0, len(char_list)) # ëœë¤ ìœ„ì¹˜ ì‚½ì…\n",
    "            char = random.choice(SPECIAL_CHARS)\n",
    "            char_list.insert(pos, char)\n",
    "        return \"\".join(char_list)\n",
    "\n",
    "    new_pre_text = insert_noise(pre_text, ratio)\n",
    "    new_post_text = insert_noise(post_text, ratio)\n",
    "    \n",
    "    # [í•µì‹¬] ì •ë‹µ ì‹œì‘ ìœ„ì¹˜ ê°±ì‹ : ëŠ˜ì–´ë‚œ ì•ë¶€ë¶„ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¡œ ì¬ì„¤ì •\n",
    "    new_answer_start = len(new_pre_text)\n",
    "    new_context = new_pre_text + ans_text_original + new_post_text\n",
    "    \n",
    "    return new_context, new_answer_start, ans_text_original\n",
    "\n",
    "# --- 3. Answer Insertion Logic (ì •ë‹µ ì˜¤ì—¼) ---\n",
    "def get_answer_insertion_noise(text, answer_text, answer_start, ratio=0.3):\n",
    "    answer_end = answer_start + len(answer_text)\n",
    "    \n",
    "    pre_text = text[:answer_start]\n",
    "    ans_chars = list(text[answer_start:answer_end])\n",
    "    post_text = text[answer_end:]\n",
    "    \n",
    "    n_insert = int(len(ans_chars) * ratio)\n",
    "    n_insert = max(1, n_insert)\n",
    "    \n",
    "    for _ in range(n_insert):\n",
    "        pos = random.randint(1, len(ans_chars)-1) if len(ans_chars) > 1 else 0\n",
    "        char = random.choice(SPECIAL_CHARS)\n",
    "        ans_chars.insert(pos, char)\n",
    "        \n",
    "    new_ans_text = \"\".join(ans_chars)\n",
    "    new_context = pre_text + new_ans_text + post_text\n",
    "    \n",
    "    # ì •ë‹µ ë‚´ë¶€ê°€ ë³€í•œ ê²ƒì´ë¯€ë¡œ ì‹œì‘ ì¸ë±ìŠ¤(answer_start)ëŠ” ë³€í•˜ì§€ ì•ŠìŒ\n",
    "    return new_context, answer_start, new_ans_text\n",
    "\n",
    "# --- í†µí•© ìƒì„± í•¨ìˆ˜ ---\n",
    "def generate_all_datasets():\n",
    "    print(f\"ğŸ”„ Loading input: {INPUT_FILE}\")\n",
    "    try:\n",
    "        source_data = load_json(INPUT_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Input file not found. Please run the preprocessing step first.\")\n",
    "        return\n",
    "\n",
    "    datasets = {\n",
    "        \"baseline\":      {\"version\": \"KorQuAD_v1.0_baseline\", \"data\": []},\n",
    "        \"deletion\":      {\"version\": \"KorQuAD_v1.0_deletion\", \"data\": []},\n",
    "        \"insertion_que\": {\"version\": \"KorQuAD_v1.0_insertion_que\", \"data\": []},\n",
    "        \"insertion_ans\": {\"version\": \"KorQuAD_v1.0_insertion_ans\", \"data\": []}\n",
    "    }\n",
    "    \n",
    "    print(\"âš¡ Processing all noise types...\")\n",
    "    \n",
    "    for doc in tqdm(source_data['data']):\n",
    "        doc_structs = {\n",
    "            k: {\"title\": doc['title'], \"paragraphs\": []} for k in datasets.keys()\n",
    "        }\n",
    "        \n",
    "        for para in doc['paragraphs']:\n",
    "            original_context = para['context']\n",
    "            \n",
    "            for qa in para['qas']:\n",
    "                ans = qa['answers'][0]\n",
    "                a_text = ans['text']\n",
    "                a_start = ans['answer_start']\n",
    "                \n",
    "                # 1. Baseline\n",
    "                doc_structs[\"baseline\"][\"paragraphs\"].append({\n",
    "                    \"context\": original_context,\n",
    "                    \"qas\": [qa]\n",
    "                })\n",
    "\n",
    "                # 2. Deletion (ë°˜í™˜ê°’ 3ê°œ ë§¤ì¹­ ìˆ˜ì • ì™„ë£Œ)\n",
    "                del_ctx, del_start, del_ans = get_deletion_noise(original_context, a_text, a_start, ratio=0.15)\n",
    "                doc_structs[\"deletion\"][\"paragraphs\"].append({\n",
    "                    \"context\": del_ctx,\n",
    "                    \"qas\": [{\n",
    "                        \"id\": qa['id'], \"question\": qa['question'],\n",
    "                        \"answers\": [{\"text\": del_ans, \"answer_start\": del_start}]\n",
    "                    }]\n",
    "                })\n",
    "                \n",
    "                # 3. Insertion Que\n",
    "                ins_q_ctx, ins_q_start, ins_q_ans = get_context_insertion_noise(original_context, a_text, a_start, ratio=0.3)\n",
    "                doc_structs[\"insertion_que\"][\"paragraphs\"].append({\n",
    "                    \"context\": ins_q_ctx,\n",
    "                    \"qas\": [{\n",
    "                        \"id\": qa['id'], \"question\": qa['question'],\n",
    "                        \"answers\": [{\"text\": ins_q_ans, \"answer_start\": ins_q_start}]\n",
    "                    }]\n",
    "                })\n",
    "                \n",
    "                # 4. Insertion Ans\n",
    "                ins_a_ctx, ins_a_start, ins_a_ans = get_answer_insertion_noise(original_context, a_text, a_start, ratio=0.3)\n",
    "                doc_structs[\"insertion_ans\"][\"paragraphs\"].append({\n",
    "                    \"context\": ins_a_ctx,\n",
    "                    \"qas\": [{\n",
    "                        \"id\": qa['id'], \"question\": qa['question'],\n",
    "                        \"answers\": [{\"text\": ins_a_ans, \"answer_start\": ins_a_start}]\n",
    "                    }]\n",
    "                })\n",
    "        \n",
    "        # ë¬¸ì„œ ì €ì¥\n",
    "        for k in datasets.keys():\n",
    "            if doc_structs[k][\"paragraphs\"]:\n",
    "                datasets[k][\"data\"].append(doc_structs[k])\n",
    "\n",
    "    print(\"ğŸ’¾ Saving files...\")\n",
    "    for key, data_content in datasets.items():\n",
    "        filename = f\"train_{key}.json\"\n",
    "        save_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        save_json(data_content, save_path)\n",
    "        print(f\"   - Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce55518-90d1-48d5-a5ab-a31a722b1f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading input: ./results/cleaned/bert_train.json\n",
      "âš¡ Processing all noise types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175807/175807 [06:47<00:00, 431.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving files...\n",
      "   - Saved: ./results/corrupted\\train_baseline.json\n",
      "   - Saved: ./results/corrupted\\train_deletion.json\n",
      "   - Saved: ./results/corrupted\\train_insertion_que.json\n",
      "   - Saved: ./results/corrupted\\train_insertion_ans.json\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤í–‰\n",
    "generate_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c6d4a-6fac-482a-8da2-6c88d760cabf",
   "metadata": {},
   "source": [
    "### ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a831d380-dc4a-47e4-97d3-ce2027554aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading Baseline Data...\n",
      "\n",
      "ğŸ” Verifying: train_baseline.json (Mode: baseline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175807/175807 [00:00<00:00, 191664.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“Š Results for train_baseline.json\n",
      "   - Total Samples: 292952\n",
      "   - âœ… Alignment OK: 292952 (100.0%)\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ” Verifying: train_deletion.json (Mode: deletion)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175807/175807 [00:01<00:00, 114380.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“Š Results for train_deletion.json\n",
      "   - Total Samples: 292952\n",
      "   - âœ… Alignment OK: 292952 (100.0%)\n",
      "   - ğŸ§ª Corruption Verified: 292952 (100.0%)\n",
      "     (â„¹ï¸ Includes 17 samples confirmed skipped due to < 7 words)\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ” Verifying: train_insertion_ans.json (Mode: insertion_ans)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175807/175807 [00:02<00:00, 81368.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“Š Results for train_insertion_ans.json\n",
      "   - Total Samples: 292952\n",
      "   - âœ… Alignment OK: 292952 (100.0%)\n",
      "   - ğŸ§ª Corruption Verified: 292952 (100.0%)\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ” Verifying: train_insertion_que.json (Mode: insertion_que)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175807/175807 [00:02<00:00, 81119.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“Š Results for train_insertion_que.json\n",
      "   - Total Samples: 292952\n",
      "   - âœ… Alignment OK: 292952 (100.0%)\n",
      "   - ğŸ§ª Corruption Verified: 292952 (100.0%)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ê²€ì¦ ëŒ€ìƒ ê²½ë¡œ\n",
    "TARGET_DIR = \"./results/corrupted\"\n",
    "BASELINE_FILE = os.path.join(TARGET_DIR, \"train_baseline.json\")\n",
    "SPECIAL_CHARS = ['!', '@', '#', '$', '%', '^', '&', '*']\n",
    "\n",
    "def load_json(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Load Failed: {filepath} ({e})\")\n",
    "        return None\n",
    "\n",
    "def build_reference_map(baseline_data):\n",
    "    ref_map = {}\n",
    "    for doc in baseline_data['data']:\n",
    "        for para in doc['paragraphs']:\n",
    "            context = para['context']\n",
    "            for qa in para['qas']:\n",
    "                qid = qa['id']\n",
    "                ans = qa['answers'][0]\n",
    "                ref_map[qid] = {\n",
    "                    \"context\": context,\n",
    "                    \"text\": ans['text'],\n",
    "                    \"start\": ans['answer_start']\n",
    "                }\n",
    "    return ref_map\n",
    "\n",
    "def verify_dataset(filepath, mode, ref_map):\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"\\nğŸ” Verifying: {filename} (Mode: {mode})\")\n",
    "    \n",
    "    data = load_json(filepath)\n",
    "    if not data: return\n",
    "\n",
    "    stats = {\n",
    "        \"total\": 0,\n",
    "        \"alignment_ok\": 0,\n",
    "        \"alignment_fail\": 0,\n",
    "        \"corruption_ok\": 0,\n",
    "        \"corruption_fail\": 0,\n",
    "        \"skipped_short\": 0,\n",
    "        \"errors\": []\n",
    "    }\n",
    "\n",
    "    for doc in tqdm(data['data'], desc=\"Checking samples\"):\n",
    "        for para in doc['paragraphs']:\n",
    "            context = para['context']\n",
    "            for qa in para['qas']:\n",
    "                stats[\"total\"] += 1\n",
    "                qid = qa['id']\n",
    "                \n",
    "                curr_ans = qa['answers'][0]\n",
    "                curr_text = curr_ans['text']\n",
    "                curr_start = curr_ans['answer_start']\n",
    "\n",
    "                # 1. Alignment Check\n",
    "                extracted = context[curr_start : curr_start + len(curr_text)]\n",
    "                if extracted == curr_text:\n",
    "                    stats[\"alignment_ok\"] += 1\n",
    "                else:\n",
    "                    stats[\"alignment_fail\"] += 1\n",
    "                    if len(stats[\"errors\"]) < 3:\n",
    "                        stats[\"errors\"].append(f\"[Align Error] ID: {qid}\")\n",
    "\n",
    "                # 2. Corruption Check\n",
    "                if mode == 'baseline':\n",
    "                    stats[\"corruption_ok\"] += 1\n",
    "                    continue\n",
    "\n",
    "                if qid not in ref_map: continue\n",
    "                ref = ref_map[qid]\n",
    "                is_corrupted = False\n",
    "                \n",
    "                if mode == 'deletion':\n",
    "                    # ê¸¸ì´ê°€ ì¤„ì—ˆìœ¼ë©´ -> ì˜¤ì—¼ ì„±ê³µ\n",
    "                    if len(context) < len(ref['context']):\n",
    "                        is_corrupted = True\n",
    "                    else:\n",
    "                        # [ê¸¸ì´ê°€ ì•ˆ ì¤„ì–´ë“  ê²½ìš° ì •ë°€ ê²€ì¦]\n",
    "                        # \"ì™œ ì•ˆ ì¤„ì—ˆë‚˜?\" -> \"ì§„ì§œë¡œ ë‹¨ì–´ê°€ 7ê°œ ë¯¸ë§Œì´ë¼ì„œ ê·¸ëŸ°ê°€?\" í™•ì¸\n",
    "                        \n",
    "                        ref_ctx = ref['context']\n",
    "                        ref_start = ref['start']\n",
    "                        ref_end = ref_start + len(ref['text'])\n",
    "                        \n",
    "                        pre_text = ref_ctx[:ref_start]\n",
    "                        post_text = ref_ctx[ref_end:]\n",
    "                        \n",
    "                        pre_tokens = [t for t in pre_text.split(' ') if t.strip()]\n",
    "                        post_tokens = [t for t in post_text.split(' ') if t.strip()]\n",
    "                        \n",
    "                        # [í•µì‹¬ ìˆ˜ì •] ê³„ì‚°ì‹ ëŒ€ì‹  ëª…ì‹œì ìœ¼ë¡œ '7ë‹¨ì–´ ë¯¸ë§Œ' ì¡°ê±´ í™•ì¸\n",
    "                        # ì•ë¶€ë¶„ê³¼ ë’·ë¶€ë¶„ì´ ëª¨ë‘ 7ë‹¨ì–´ ë¯¸ë§Œì´ì–´ì•¼ ì‚­ì œê°€ 0ê°œê°€ ë¨\n",
    "                        if len(pre_tokens) < 7 and len(post_tokens) < 7:\n",
    "                            is_corrupted = True # ì§§ì•„ì„œ ìŠ¤í‚µëœ ê²ƒì´ ë§ìŒ (ì •ìƒ)\n",
    "                            stats[\"skipped_short\"] += 1\n",
    "                        else:\n",
    "                            # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ 7ë‹¨ì–´ ì´ìƒì¸ë° ê¸¸ì´ê°€ ê·¸ëŒ€ë¡œë¼ë©´ -> ì˜¤ì—¼ ì‹¤íŒ¨ (Generator ë²„ê·¸)\n",
    "                            is_corrupted = False\n",
    "                    \n",
    "                elif mode == 'insertion_que':\n",
    "                    has_special = any(c in context for c in SPECIAL_CHARS)\n",
    "                    is_longer = len(context) > len(ref['context'])\n",
    "                    if has_special and is_longer:\n",
    "                        is_corrupted = True\n",
    "                        \n",
    "                elif mode == 'insertion_ans':\n",
    "                    has_special = any(c in curr_text for c in SPECIAL_CHARS)\n",
    "                    is_longer = len(curr_text) > len(ref['text'])\n",
    "                    if has_special and is_longer:\n",
    "                        is_corrupted = True\n",
    "                \n",
    "                if is_corrupted:\n",
    "                    stats[\"corruption_ok\"] += 1\n",
    "                else:\n",
    "                    stats[\"corruption_fail\"] += 1\n",
    "\n",
    "    print(f\"   ğŸ“Š Results for {filename}\")\n",
    "    print(f\"   - Total Samples: {stats['total']}\")\n",
    "    print(f\"   - âœ… Alignment OK: {stats['alignment_ok']} ({(stats['alignment_ok']/stats['total'])*100:.1f}%)\")\n",
    "    \n",
    "    if mode != 'baseline':\n",
    "        print(f\"   - ğŸ§ª Corruption Verified: {stats['corruption_ok']} ({(stats['corruption_ok']/stats['total'])*100:.1f}%)\")\n",
    "        if mode == 'deletion' and stats['skipped_short'] > 0:\n",
    "             print(f\"     (â„¹ï¸ Includes {stats['skipped_short']} samples confirmed skipped due to < 7 words)\")\n",
    "\n",
    "        if stats['corruption_fail'] > 0:\n",
    "            print(f\"   - âš ï¸ Unchanged Samples (Fail): {stats['corruption_fail']}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(BASELINE_FILE):\n",
    "        print(\"âŒ Baseline file not found.\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸ“¥ Loading Baseline Data...\")\n",
    "    baseline_data = load_json(BASELINE_FILE)\n",
    "    ref_map = build_reference_map(baseline_data)\n",
    "    \n",
    "    files = glob.glob(os.path.join(TARGET_DIR, \"train_*.json\"))\n",
    "    files.sort()\n",
    "\n",
    "    for file_path in files:\n",
    "        mode = os.path.basename(file_path).replace(\"train_\", \"\").replace(\".json\", \"\")\n",
    "        verify_dataset(file_path, mode, ref_map)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34921d0e-e5e9-493e-8f74-e10b32831866",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
